{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQM1MNibuN1Hzna2OGwRRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishant-pitta/DANN-code/blob/main/MSNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform_mnist = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "transform_mnistm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tj6Ubgegfe2",
        "outputId": "ac2ff8fc-bd58-4db4-9bcd-2ca4fa6b2080"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 34277683.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1156294.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9661994.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7221580.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST-M dataset\n",
        "def load_pickle_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f, encoding='latin1')\n",
        "    return data\n",
        "\n",
        "mnistm = load_pickle_file('data/mnist-m.pkl')\n",
        "\n",
        "# Inspect the structure of the loaded data\n",
        "print(type(mnistm))\n",
        "print(mnistm.keys())\n",
        "\n",
        "print(type(mnistm['train']))\n",
        "print(type(mnistm['test']))\n",
        "\n",
        "print(mnistm['train'].shape)\n",
        "print(mnistm['test'].shape)\n",
        "\n",
        "print(type(mnistm['train'][0]))\n",
        "print(type(mnistm['test'][0]))\n",
        "\n",
        "print(mnistm['train'][0])\n",
        "print(mnistm['test'][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSJ99eZLS3Wu",
        "outputId": "2991810b-bc42-4ef7-a263-f0c2f5f0bf12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys(['test', 'train'])\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(60000, 28, 28, 3)\n",
            "(10000, 28, 28, 3)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[[ 60  77 133]\n",
            "  [ 61  78 134]\n",
            "  [ 62  79 135]\n",
            "  ...\n",
            "  [ 57  76 132]\n",
            "  [ 57  76 132]\n",
            "  [ 57  77 130]]\n",
            "\n",
            " [[ 60  77 133]\n",
            "  [ 61  78 134]\n",
            "  [ 61  78 134]\n",
            "  ...\n",
            "  [ 57  76 132]\n",
            "  [ 57  76 132]\n",
            "  [ 57  76 132]]\n",
            "\n",
            " [[ 61  78 134]\n",
            "  [ 61  78 134]\n",
            "  [ 61  78 134]\n",
            "  ...\n",
            "  [ 56  75 131]\n",
            "  [ 56  75 131]\n",
            "  [ 57  76 132]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 57  76 134]\n",
            "  [ 57  77 130]\n",
            "  [ 57  77 130]\n",
            "  ...\n",
            "  [  6  34  35]\n",
            "  [  6  35  33]\n",
            "  [  7  36  32]]\n",
            "\n",
            " [[ 58  77 135]\n",
            "  [ 58  77 133]\n",
            "  [ 57  77 130]\n",
            "  ...\n",
            "  [  9  37  38]\n",
            "  [  8  37  35]\n",
            "  [  8  37  35]]\n",
            "\n",
            " [[ 57  73 133]\n",
            "  [ 62  78 137]\n",
            "  [ 58  75 131]\n",
            "  ...\n",
            "  [  8  37  33]\n",
            "  [  8  37  35]\n",
            "  [  8  37  35]]]\n",
            "[[[ 63  78 133]\n",
            "  [ 59  76 132]\n",
            "  [ 65  81 140]\n",
            "  ...\n",
            "  [ 15  40  45]\n",
            "  [  8  37  35]\n",
            "  [  8  37  35]]\n",
            "\n",
            " [[ 62  79 135]\n",
            "  [ 61  80 136]\n",
            "  [ 61  80 138]\n",
            "  ...\n",
            "  [ 11  35  45]\n",
            "  [  9  34  38]\n",
            "  [ 12  41  39]]\n",
            "\n",
            " [[ 61  78 132]\n",
            "  [ 63  82 138]\n",
            "  [ 56  75 131]\n",
            "  ...\n",
            "  [ 13  36  54]\n",
            "  [ 10  35  42]\n",
            "  [  9  37  38]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 73  86 130]\n",
            "  [ 70  82 130]\n",
            "  [ 66  78 128]\n",
            "  ...\n",
            "  [ 32  53  72]\n",
            "  [ 41  61  86]\n",
            "  [ 10  31  52]]\n",
            "\n",
            " [[ 62  75 119]\n",
            "  [ 47  60 105]\n",
            "  [ 44  56 104]\n",
            "  ...\n",
            "  [ 39  60  81]\n",
            "  [ 30  50  77]\n",
            "  [  7  27  52]]\n",
            "\n",
            " [[ 23  36  78]\n",
            "  [ 32  45  90]\n",
            "  [ 51  63 111]\n",
            "  ...\n",
            "  [ 20  41  62]\n",
            "  [ 37  57  84]\n",
            "  [ 28  49  76]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform_mnist = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "transform_mnistm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load MNIST-M dataset\n",
        "def load_pickle_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f, encoding='latin1')\n",
        "    return data\n",
        "\n",
        "mnistm = load_pickle_file('data/mnist-m.pkl')\n",
        "mnistm_train_images = mnistm['train']\n",
        "mnistm_test_images = mnistm['test']\n",
        "\n",
        "# Labels are inferred from the MNIST dataset as they are digits from 0 to 9\n",
        "mnistm_train_labels = np.tile(np.arange(10), 6000)\n",
        "mnistm_test_labels = np.tile(np.arange(10), 1000)\n",
        "\n",
        "class MNISTMDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.images[idx], self.labels[idx]\n",
        "        img = img.transpose(2, 0, 1)  # Change to [3, 28, 28]\n",
        "        img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "        img = torch.tensor(img)  # Convert to tensor\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "mnistm_train_dataset = MNISTMDataset(mnistm_train_images, mnistm_train_labels, transform=None)\n",
        "mnistm_test_dataset = MNISTMDataset(mnistm_test_images, mnistm_test_labels, transform=None)\n",
        "mnistm_train_loader = DataLoader(mnistm_train_dataset, batch_size=64, shuffle=True)\n",
        "mnistm_test_loader = DataLoader(mnistm_test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(CNN, self).__init__()\n",
        "        # Convolutional and pooling layers\n",
        "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 48, kernel_size=5, stride=1, padding=2)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(48*7*7, 100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.pool(self.conv1(x)))\n",
        "        x = self.relu(self.pool(self.conv2(x)))\n",
        "        x = x.view(-1, 48*7*7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CNN(input_channels=3)  # Updated to handle 3-channel inputs\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function (with tqdm progress bar)\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=1):  # Set num_epochs to 1 for quick testing\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Train the CNN on MNIST\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluate the CNN on MNIST and MNIST-M\n",
        "mnist_accuracy = evaluate_model(model, test_loader)\n",
        "mnistm_accuracy = evaluate_model(model, mnistm_test_loader)\n",
        "\n",
        "\n",
        "print(f'Test Accuracy on MNIST: {mnist_accuracy:.2f}%')\n",
        "print(f'Test Accuracy on MNIST-M: {mnistm_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4byIpIp5JDrS",
        "outputId": "735a18bb-491a-4359-ce08-446cb528296a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 938/938 [01:56<00:00,  8.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Loss: 0.1512\n",
            "Test Accuracy on MNIST: 98.51%\n",
            "Test Accuracy on MNIST-M: 9.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.autograd as autograd\n",
        "\n",
        "class GradientReversalLayer(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.alpha, None\n"
      ],
      "metadata": {
        "id": "pfFQm6wUUzNA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DANN(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super(DANN, self).__init__()\n",
        "        # Feature extractor (green blocks)\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 48, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Class classifier (purple blocks)\n",
        "        self.class_classifier = nn.Sequential(\n",
        "            nn.Linear(48*7*7, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 10)\n",
        "        )\n",
        "\n",
        "        # Domain classifier (dark pink blocks)\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.Linear(48*7*7, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 1),\n",
        "            nn.Sigmoid()  # Logistic regression\n",
        "        )\n",
        "\n",
        "    def forward(self, x, alpha=1.0):\n",
        "        features = self.feature_extractor(x)\n",
        "        features = features.view(-1, 48*7*7)\n",
        "        reverse_features = GradientReversalLayer.apply(features, alpha)\n",
        "        class_output = self.class_classifier(features)\n",
        "        domain_output = self.domain_classifier(reverse_features)\n",
        "        return class_output, domain_output"
      ],
      "metadata": {
        "id": "d50r4EsFU3Ne"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function for DANN\n",
        "def train_dann(model, source_loader, target_loader, criterion_class, criterion_domain, optimizer, num_epochs=1, alpha=0.1):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss_class = 0.0\n",
        "        running_loss_domain = 0.0\n",
        "        num_batches = min(len(source_loader), len(target_loader))\n",
        "        for (source_data, source_labels), (target_data, _) in tqdm(zip(source_loader, target_loader), total=num_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            # Combine source and target data\n",
        "            combined_data = torch.cat((source_data, target_data), 0)\n",
        "            combined_labels = torch.cat((source_labels, source_labels), 0)  # Dummy labels for target data\n",
        "            domain_labels = torch.cat((torch.zeros(source_data.size(0)), torch.ones(target_data.size(0))), 0).float().view(-1, 1)  # Ensure proper shape\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            class_output, domain_output = model(combined_data, alpha)\n",
        "\n",
        "            # Calculate losses\n",
        "            loss_class = criterion_class(class_output[:source_data.size(0)], source_labels)\n",
        "            loss_domain = criterion_domain(domain_output, domain_labels)\n",
        "            loss = loss_class + loss_domain\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss_class += loss_class.item() * source_data.size(0)\n",
        "            running_loss_domain += loss_domain.item() * combined_data.size(0)\n",
        "\n",
        "        epoch_loss_class = running_loss_class / len(source_loader.dataset)\n",
        "        epoch_loss_domain = running_loss_domain / (len(source_loader.dataset) + len(target_loader.dataset))\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Class Loss: {epoch_loss_class:.4f}, Domain Loss: {epoch_loss_domain:.4f}')\n",
        "\n",
        "# Evaluation function for DANN\n",
        "def evaluate_dann(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            class_output, _ = model(images)\n",
        "            _, predicted = torch.max(class_output.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Initialize the DANN model\n",
        "dann_model = DANN(input_channels=3)\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_domain = nn.BCELoss()  # Binary Cross-Entropy Loss for domain classifier\n",
        "optimizer = optim.Adam(dann_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the DANN model for one epoch\n",
        "train_dann(dann_model, train_loader, mnistm_train_loader, criterion_class, criterion_domain, optimizer, num_epochs=1, alpha=0.1)\n",
        "\n",
        "# Evaluate the DANN model on MNIST and MNIST-M\n",
        "mnist_accuracy_dann = evaluate_dann(dann_model, test_loader)\n",
        "mnistm_accuracy_dann = evaluate_dann(dann_model, mnistm_test_loader)\n",
        "print(f'Test Accuracy on MNIST with DANN: {mnist_accuracy_dann:.2f}%')\n",
        "print(f'Test Accuracy on MNIST-M with DANN: {mnistm_accuracy_dann:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcn3_FHWU6UC",
        "outputId": "bc5ede1f-277d-4848-c2ff-d6e064416990"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 938/938 [03:32<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Class Loss: 0.2003, Domain Loss: 0.1775\n",
            "Test Accuracy on MNIST with DANN: 98.41%\n",
            "Test Accuracy on MNIST-M with DANN: 9.90%\n"
          ]
        }
      ]
    }
  ]
}